{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import math\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Data\n",
    "def fill_na(df):\n",
    "    df.fillna(df.median(), inplace= True)\n",
    "\n",
    "    \n",
    "#Preparing data for use\n",
    "def date_prep(df):\n",
    "    df['year_sold'] = df['date'].map(lambda x: x[-4:])\n",
    "    df['month'] = df['date'].map(lambda x: x[0:2])\n",
    "    df['month'] = df['month'].map(lambda x: x.replace('/',''))\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    df['year_sold'] = df['year_sold'].astype(int)\n",
    "\n",
    "#Binning Data\n",
    "def date_bins(df):    \n",
    "    bins = [0, 3, 6, 9, 12]\n",
    "    date_bins = pd.cut(df['month'], bins, include_lowest=True, labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
    "    date_bins = date_bins.cat.as_unordered()\n",
    "    df[\"season\"] = date_bins\n",
    "    df = df.drop([\"month\",\"date\"], axis = 1, inplace= True)\n",
    "\n",
    "#Function to clean data\n",
    "def data_cleaning(df):\n",
    "    df = df[(df[\"price\"] > 100000) & (df[\"price\"]< 1500000)]\n",
    "    df = df[(np.abs(stats.zscore(df[\"sqft_living\"])) < 3)]\n",
    "    df = (df[df[\"bedrooms\"] < 7])\n",
    "    df = (df[df[\"bathrooms\"] < 6])\n",
    "    df = (df[df[\"sqft_living\"] < 8000])\n",
    "        \n",
    "#Function to convert categorical data \n",
    "# def convert_categorical(column_names):\n",
    "#     categorical_encoder = preprocessing.OneHotEncoder(handle_unknown= \"ignore\")\n",
    "#     cat_df = categorical_encoder.fit_transform([column_names])\n",
    "\n",
    "#OLS regression function    \n",
    "def OLS_reg(df):\n",
    "    y = df[\"price\"]\n",
    "    X = df.drop(\"price\", axis=1)\n",
    "    model = sm.OLS(y, sm.add_constant(X), missing = \"drop\").fit()\n",
    "    results = model.summary()\n",
    "    print(results)\n",
    "\n",
    "#Predrictive model function\n",
    "def predictive_model(df):\n",
    "    df2 = df.to_numpy()\n",
    "    y = df2[:, 0]\n",
    "    X = df2[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    data_transformation = preprocessing.StandardScaler()\n",
    "    data = data_transformation.fit_transform(X_train)\n",
    "    model = LinearRegression().fit(data, y_train)\n",
    "    accuracy = model.score(data, y_train)\n",
    "    test_accuracy = model.score(data_transformation.transform((X_test)), y_test)#change accuracy\n",
    "    y_pred = model.predict(data_transformation.transform(X_test))\n",
    "    print(\"X_train shape\", X_train.shape)\n",
    "    print(\"X_test shape\",X_test.shape)\n",
    "    print(\"y_train shape\",y_train.shape)\n",
    "    print(\"y_test shape\",y_test.shape)\n",
    "    print(\"R^2\",accuracy)\n",
    "    print(\"Test R^2\",test_accuracy)\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    model1 = pd.DataFrame({'Actual value': y_test, 'Predicted value': (y_pred.round(2)), \"Difference\" : (abs(y_test-y_pred)).round(2)})\n",
    "    print(model1.sample(10).head(10))\n",
    "    true_value = model1[\"Actual value\"]\n",
    "    predicted_value = model1[\"Predicted value\"]\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.scatter(true_value, predicted_value, c='crimson')\n",
    "    # plt.yscale('log')\n",
    "    # plt.xscale('log')\n",
    "    p1 = min(min(predicted_value), min(true_value))\n",
    "    p2 = max(max(predicted_value), max(true_value))\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('Predictions''True Values', fontsize=15)\n",
    "    plt.ylabel('True Values', fontsize=15)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "#Prediction model for logged data\n",
    "def predictive_model_log(df):\n",
    "    df2 = df.to_numpy()\n",
    "    y = df2[:, 0]\n",
    "    X = df2[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler  = preprocessing.StandardScaler()\n",
    "    data = scaler.fit_transform(X_train)\n",
    "    model = LinearRegression().fit(data, y_train)\n",
    "    accuracy = model.score(data, y_train)\n",
    "    test_accuracy = model.score(scaler.transform((X_test)), y_test)#change accuracy\n",
    "    y_pred = model.predict(scaler.transform(X_test))\n",
    "    y_pred = np.exp(y_pred)\n",
    "    y_test = np.exp(y_test)\n",
    "    print(\"X_train shape\", X_train.shape)\n",
    "    print(\"X_test shape\",X_test.shape)\n",
    "    print(\"y_train shape\",y_train.shape)\n",
    "    print(\"y_test shape\",y_test.shape)\n",
    "    print(\"R^2\",accuracy)\n",
    "    print(\"Test R^2\",test_accuracy)\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    model1 = pd.DataFrame({'Actual value': y_test, 'Predicted value': (y_pred.round(2)), \"Difference\" : (abs(y_test-y_pred)).round(2)})\n",
    "    print(model1.sample(10).head(10))\n",
    "    true_value = model1[\"Actual value\"]\n",
    "    predicted_value = model1[\"Predicted value\"]\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.scatter(true_value, predicted_value, c='crimson')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    p1 = min(min(predicted_value), min(true_value))\n",
    "    p2 = max(max(predicted_value), max(true_value))\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('Predictions''True Values', fontsize=15)\n",
    "    plt.ylabel('True Values', fontsize=15)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "#Polynomial prediction function\n",
    "def poly_model(df):\n",
    "    df2 = df.to_numpy()\n",
    "    y = df2[:, 0]\n",
    "    X = df2[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    data_transformation = preprocessing.StandardScaler()\n",
    "    data = data_transformation.fit_transform(X_train)\n",
    "    model = LinearRegression().fit(data, y_train)\n",
    "    for index, degree in enumerate([2,3]):  \n",
    "    # Instantiate PolynomialFeatures\n",
    "        poly = PolynomialFeatures(degree)\n",
    "    # Fit and transform X_train\n",
    "        X_poly_train = poly.fit_transform(X_train)\n",
    "    # Instantiate and fit a linear regression model to the polynomial transformed train features\n",
    "        reg_poly = LinearRegression().fit(X_poly_train, y_train)\n",
    "    # Transform the test data into polynomial features\n",
    "        X_poly_test = poly.transform(X_test)\n",
    "    # Get predicted values for transformed polynomial test data  \n",
    "        y_pred = reg_poly.predict(X_poly_test)\n",
    "    # Evaluate model performance on test data\n",
    "        print(\"Train degree %d\" % degree, reg_poly.score(X_poly_train, y_train))\n",
    "        print(\"Test degree %d\" % degree, reg_poly.score(X_poly_test, y_test))\n",
    "        print(\"train degree %d\" % degree, reg_poly.score(X_poly_train, y_train))\n",
    "\n",
    "#Polynomial prediction function for logged data\n",
    "def poly_model_log(df):\n",
    "    df2 = df.to_numpy()\n",
    "    y = df2[:, 0]\n",
    "    X = df2[:,1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    data_transformation = preprocessing.StandardScaler()\n",
    "    data = data_transformation.fit_transform(X_train)\n",
    "    model = LinearRegression().fit(data, y_train)\n",
    "    for index, degree in enumerate([2,3]):  \n",
    "    # Instantiate PolynomialFeatures\n",
    "        poly = PolynomialFeatures(degree)\n",
    "    # Fit and transform X_train\n",
    "        X_poly_train = poly.fit_transform(X_train)\n",
    "    # Instantiate and fit a linear regression model to the polynomial transformed train features\n",
    "        reg_poly = LinearRegression().fit(X_poly_train, y_train)\n",
    "    # Transform the test data into polynomial features\n",
    "        X_poly_test = poly.transform(X_test)\n",
    "    # Get predicted values for transformed polynomial test data  \n",
    "        y_pred = reg_poly.predict(X_poly_test)\n",
    "        y_pred = np.exp(y_pred)\n",
    "        y_test = np.exp(y_test)\n",
    "    # Evaluate model performance on test data\n",
    "        print(\"Train degree %d\" % degree, reg_poly.score(X_poly_train, y_train))\n",
    "        print(\"Test degree %d\" % degree, reg_poly.score(X_poly_test, y_test))\n",
    "        print(\"train degree %d\" % degree, reg_poly.score(X_poly_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data\n",
    "#Read dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "df1 = pd.read_csv(\"data/kc_house_data.csv\", index_col=\"id\")\n",
    "df_income = pd.read_csv(\"data/dc_housing_income_by_zip.csv\")#source https://www.kaggle.com/miker400/washington-state-home-mortgage-hdma2016?select=Washington_State_HDMA-2016.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using median income per zipcode\n",
    "income = df_income.groupby(\"zipcode\").median().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping last value in dataset\n",
    "df1 = df1[~df1.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging datasets on zipcode\n",
    "df = df1.merge(income, on =\"zipcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running functions to change date to usable format and bin dates into seasons\n",
    "date_prep(df)\n",
    "date_bins(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes to split the grade and extract the number and dropping original grade column\n",
    "new = df[\"grade\"].str.split(\" \", n = 1, expand = True)\n",
    "df[\"grade_1\"]=new[0]\n",
    "df.drop(\"grade\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking value counts in view\n",
    "df['view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating view from rating to number \n",
    "df['view'] = df['view'].map({'NONE': 1, 'AVERAGE': 2, 'GOOD': 3, 'FAIR': 4, 'EXCELLENT': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking valuecounts in waterfron\n",
    "df['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating waterfron to number\n",
    "df['waterfront'] = df['waterfront'].map({'YES': 1, 'NO': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking value counts in condition\n",
    "df['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating condition to number\n",
    "df['condition'] = df['condition'].map({'Poor': 1, 'Fair': 2, 'Average': 3, 'Good': 4, 'Very Good': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking value counts in season\n",
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chaning season to number\n",
    "df['season'] = df['season'].map({'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for NAN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing values with median values of each column\n",
    "df['waterfront'].fillna((df['waterfront'].median()), inplace=True)\n",
    "df['grade_1'].fillna((df['grade_1'].median()), inplace=True)\n",
    "df['yr_renovated'].fillna((df['yr_renovated'].median()), inplace=True)\n",
    "df['view'].fillna((df['view'].median()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking NAN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking variable types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting variables to floats \n",
    "df[\"sqft_basement\"] = df[\"sqft_basement\"].replace(\"?\",\"0\").astype(float)\n",
    "df[\"season\"] = df[\"season\"].astype(float)\n",
    "df[\"grade_1\"] = df[\"grade_1\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking correlation between variables\n",
    "initial_corr = df.corr().sort_values(by=\"price\", ascending=False)\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.heatmap(initial_corr, linewidth=0.5)\n",
    "plt.show()\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for highest correlation\n",
    "high_corr_formula = 'price ~ sqft_living'\n",
    "high_corr_formula = ols(formula=high_corr_formula, data=df).fit()\n",
    "singular_model = high_corr_formula.summary()\n",
    "print(singular_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First OLS model\n",
    "OLS_reg(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Predictive model\n",
    "predictive_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting data to check for correlation and outliers\n",
    "sns.pairplot(df, y_vars=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the charts there are some outliers that need to be addressed, after the outliers are addressed we will also need to log tranform some of our data to turn into normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking data for outliers\n",
    "#Check price\n",
    "#Check bedroom \n",
    "#Check sqft_living\n",
    "#Check sqft_lot15\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot('price', vert=False, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"price\"] < 4000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot('sqft_living', vert=False, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping outliers from sqft_living\n",
    "df = df = df[df[\"sqft_living\"] < 8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot('bedrooms', vert=False, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing 33 with 3 as it seems to be a typo\n",
    "df[\"bedrooms\"] = df[\"bedrooms\"].replace(33,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot('bedrooms', vert=False, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bedrooms\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping houses above 9 bedrooms\n",
    "df = (df[df[\"bedrooms\"] < 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for multicollinearity  \n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(15,15))\n",
    "ax = sns.heatmap(corr, linewidth=0.5,annot = True)\n",
    "plt.show()\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "#High Colinearity to price : sqft living, grade, bathrooms, view, lat, bedrooms, basement, floors, waterfront\n",
    "#sqft_living:sqft above, sqftliving15,bathrooms(possibly )\n",
    "#bathrooms\n",
    "#view\n",
    "#waterfront"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = df[[\"view\",\"waterfront\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat =pd.get_dummies(df_categorical, columns=[\"view\",\"waterfront\"], drop_first= True) \n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = df[[\"price\", \"sqft_living\",\"bathrooms\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_numerical, df_cat], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with outliers removed\n",
    "OLS_reg(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictive model with outliers removed\n",
    "predictive_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerical = df[[\"price\", \"sqft_living\",\"bathrooms\"]].copy().apply(lambda x: np.log(abs(x+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.concat([df_numerical, df_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transformed \n",
    "OLS_reg(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running predictive model for logged data\n",
    "predictive_model_log(log_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Running OLS regression on selected features \n",
    "OLS_reg(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running prediction model on selected features\n",
    "predictive_model(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running polynomial function for selected features \n",
    "poly_model(log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad65dd45c2d07e74b12869848a281d31b75ca788164d45805ca064cbe1f6cb6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
